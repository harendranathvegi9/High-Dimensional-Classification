\subsection{Dimensionality}
\paragraph{The Curse of Dimensionality} % (fold)
\label{par:paragraph_name}
The set of known problems dealing with having extremely large feature sizes are known as `The Curse of Dimensionality'. As the number of features one deals with increases, problems arise when trying to apply standard machine learning methods. The first, and most intuitive of the problems with a a large number of dimensions, is that training and testing speeds can be prohibitive. For models such as K-Nearest-Neighbors that need to calculate metrics for each example, having large feature sizes can be daunting. One additional problem with a large feature size is that as the number of features increases, the variance between distances between the points decreases, leading to less reliable results. Beyer et. al. stated the problem that as dimensionality increases the distance of the nearest neighbor to the farthest neighbor becomes small ~\cite{beyer1999nearest}. Beyer et. al. shows this by proving that the distance to the farthest point converges in probability to the closest point multiplied by a factor of $(1+\epsilon)$ for all $\epsilon > 0$, formally:
\begin{equation}
	\lim_{m\rightarrow \infty} P[DMAX_m \le (1+\epsilon)DMIN_m]=1
\end{equation}
% paragraph paragraph_name (end)
\paragraph{Johnson-Lindenstauss} % (fold)
\label{par:johnson_lindenstauss}
One aspect that can be derived from the concept of the curse of dimensionality is that there would be a benefit in decreasing the size of the feature space if the same amount of information can be preserved. The summarization of a feature space into a lower dimensional space is called dimensionality reduction. Johnson and Lindenstauss have shown in the lemma named after them that a set of points can be projected into a smaller dimensional space from a larger one such that the point's distances are just about preserved ~\cite{johnson1984extensions}. Formally, if $f$ is a mapping $:\mathbb{R}^n \rightarrow \mathbb{R}^k$ with probability $1-n^{-\beta}$ ,  $\forall u,v \in P$:
\begin{equation}
	(1 - \epsilon)||u - v||^2 \le ||f(u)-f(v)||^2 \le (1+\epsilon)||u-v||^2
\end{equation}  ~\cite{achlioptas2001database} The two methods that were used in this project for performing dimensionality reduction are Principal Component Analysis (PCA) using Singular Value Decomposition (SVD) as well the method of random projections.
% paragraph johnson_lindenstauss (end)
\paragraph{Singular Value Decomposition} % (fold)
\label{par:singular_value_decomposition}
The method of Singular Value Decomposition decomposes a data matrix in the form $A = U \Sigma V^T$, where 
if $A$ is of size $m \times n$, $U$ is an orthogonal matrix of size $m \times p$, $\Sigma$ is a diagonal matrix of size $p \times p$, and $V$ is an orthogonal matrix of size $n \times p$. If we take a limited of the the projection matrices in the form $A_k = \sum_{i=1}^k\sigma_iu_iv_i^T$ the resulting matrix is called the optimal rank-k approximation of the original matrix. ~\cite{LecturePCA}.
% paragraph singular_value_decomposition (end)
\paragraph{Random Projections} % (fold)
\label{par:random_projections}
The main takeaway from the Johnson-Lindenstauss lemma is that if one projects the data matrix using a matrix of orthogonal vectors then distances are just about preserved. Orthogonalization is a costly task, which often defeats the purpose of performing random projections to increase speed performances. Fortunately, it has been shown that with high dimensional spaces there is a high chance that a vector is nearly orthogonal, and thus it has been shown that by generating random matrices and projecting down to a lower dimensional space using the random matrices one can preserve distances ~\cite{sulic2010dimensionality}. We used the form of random projections developed by Achlioptas in the form:
	\begin{equation}
		r_{ij} = \sqrt{3} \times \left\{
			\begin{array}{ll}
				+1  & \mbox{with probability } \frac{1}{6} \\
				+0  & \mbox{with probability } \frac{2}{3} \\
				-1  & \mbox{with probability } \frac{1}{6}
			\end{array}
		\right.
	\end{equation} ~\cite{achlioptas2001database}
% paragraph random_projections (end)